{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-documentai\n",
      "  Downloading google_cloud_documentai-2.34.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pypdf in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (5.0.1)\n",
      "Requirement already satisfied: openai in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (1.52.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai)\n",
      "  Downloading google_api_core-2.21.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 (from google-cloud-documentai)\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-documentai)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-cloud-documentai)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from pypdf) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: idna>=2.8 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (2.32.3)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai)\n",
      "  Downloading grpcio-1.67.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai)\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: certifi in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nagix/projects/langchain-app/venv/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (2.2.3)\n",
      "Downloading google_cloud_documentai-2.34.0-py2.py3-none-any.whl (329 kB)\n",
      "Downloading google_api_core-2.21.0-py3-none-any.whl (156 kB)\n",
      "Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading grpcio-1.67.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.67.0-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, grpcio-status, google-auth, google-api-core, google-cloud-documentai\n",
      "Successfully installed cachetools-5.5.0 google-api-core-2.21.0 google-auth-2.35.0 google-cloud-documentai-2.34.0 googleapis-common-protos-1.65.0 grpcio-1.67.0 grpcio-status-1.67.0 proto-plus-1.25.0 protobuf-5.28.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 rsa-4.9\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-documentai pypdf openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Literal\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "from pydantic import BaseModel, SecretStr\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai\n",
    "from google.oauth2 import service_account\n",
    "from pypdf import PdfReader\n",
    "import openai\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Config\n",
    "GCP_PROJECT_ID = \"...\"\n",
    "GCP_LOCATION = \"us\"\n",
    "GCP_PROCESSOR_ID = \"...\"\n",
    "GCP_CREDENTIAL_PATH = \"...\"\n",
    "\n",
    "# single file\n",
    "file_path = \"../data/pdf/EasyRAG: Efficient Retrieval-Augmented Generation Framework for Automated Network Operations.pdf\"\n",
    "\n",
    "# multiple files\n",
    "pdf_path = \"../data/pdf\"\n",
    "\n",
    "mime_type = \"application/pdf\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCPConfig(BaseModel):\n",
    "    GCP_PROJECT_ID: str\n",
    "    GCP_LOCATION: Literal[\"us\", \"eu\"]\n",
    "    GCP_PROCESSOR_ID: str\n",
    "    GCP_CREDENTIAL_PATH: str\n",
    "\n",
    "class DocumentAI:\n",
    "    def __init__(self, filepath, gcp_config: GCPConfig, mime_type=\"application/pdf\"):\n",
    "        self.filepath = filepath\n",
    "        self.mime_type = mime_type\n",
    "        self.client = documentai.DocumentProcessorServiceClient(\n",
    "            client_options=self._get_client_options(gcp_config), \n",
    "            credentials=self._get_credentials(gcp_config)\n",
    "        )\n",
    "        self.process_name = self._get_process_name(gcp_config)\n",
    "        self.load_file()\n",
    "\n",
    "    def load_file(self):\n",
    "        with open(self.filepath, \"rb\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        self.raw_document = documentai.RawDocument(\n",
    "            content=content, mime_type=self.mime_type\n",
    "        )\n",
    "        return self.raw_document\n",
    "\n",
    "    def run_ocr(self, pages):\n",
    "        if self.raw_document is None:\n",
    "            self.load_file()\n",
    "\n",
    "        process_options = documentai.ProcessOptions(\n",
    "            individual_page_selector=documentai.ProcessOptions.IndividualPageSelector(\n",
    "                pages=pages\n",
    "            )\n",
    "        )\n",
    "\n",
    "        request = documentai.ProcessRequest(\n",
    "            name=self.process_name,\n",
    "            raw_document=self.raw_document,\n",
    "            process_options=process_options,\n",
    "        )\n",
    "\n",
    "        result = self.client.process_document(request=request)\n",
    "        return result\n",
    "\n",
    "    def _get_client_options(self, gcp_config):\n",
    "        return ClientOptions(\n",
    "            api_endpoint=f\"{gcp_config.GCP_LOCATION}-documentai.googleapis.com\"\n",
    "        )\n",
    "    \n",
    "    def _get_credentials(self, gcp_config):\n",
    "        return service_account.Credentials.from_service_account_file(\n",
    "            gcp_config.GCP_CREDENTIAL_PATH\n",
    "        )\n",
    "    \n",
    "    def _get_process_name(self, gcp_config):\n",
    "        return self.client.processor_path(\n",
    "            gcp_config.GCP_PROJECT_ID,\n",
    "            gcp_config.GCP_LOCATION,\n",
    "            gcp_config.GCP_PROCESSOR_ID\n",
    "        )\n",
    "\n",
    "class DocumentParser:\n",
    "    def __init__(self, document):\n",
    "        self.document = document\n",
    "        self.text = document.text\n",
    "\n",
    "    @property\n",
    "    def pages(self):\n",
    "        return self.document.pages\n",
    "        \n",
    "    def get_content_by_page(self, page_number):\n",
    "        p = None\n",
    "        for page in self.pages:\n",
    "            if page.page_number == page_number:\n",
    "                p = page\n",
    "                break\n",
    " \n",
    "        if p is None:\n",
    "            raise ValueError()\n",
    "\n",
    "        content = \"\"\n",
    "\n",
    "        for line in p.lines:\n",
    "            for segment in line.layout.text_anchor.text_segments:\n",
    "                content += self.text[segment.start_index:segment.end_index]\n",
    "        \n",
    "        return content\n",
    "\n",
    "    def get_image_by_page(self, page_number):\n",
    "        p = None\n",
    "        for page in self.pages:\n",
    "            if page.page_number == page_number:\n",
    "                p = page\n",
    "                break\n",
    "\n",
    "        if p is None:\n",
    "            raise ValueError()\n",
    "\n",
    "        return p.image.content\n",
    "    \n",
    "def refine(ocr_processed, image):\n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    prompt = f\"\"\"# Role\n",
    "You are an expert at comparing and refining OCR processing and images.\n",
    "Given an image and OCR processing, refine the OCR processing and answer the question.\n",
    "If a table exists, convert it to markdown and answer.\n",
    "Do not include ``` or ```markdown in your answer.\n",
    "** Don't add anything else and only answer the refined content. **\n",
    "# ocr_processed\n",
    "{ocr_processed}\n",
    "# image\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64, {image}\"}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text\n",
    "\n",
    "def transform_to_base64(byte):\n",
    "    import base64\n",
    "    return base64.b64encode(byte).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "arXiv:2410.10315v2 [cs.CL] 15 Oct 2024  \n",
       "EasyRAG: Efficient Retrieval-Augmented Generation Framework for Automated Network Operations  \n",
       "Zhangchi Feng¹, Dongdong Kuang¹, Zhongyuan Wang¹, Zhijie Nie¹, Yaowei Zheng¹, Richong Zhang¹*  \n",
       "¹CCSE, School of Computer Science and Engineering, Beihang University, Beijing, China  \n",
       "{zcmuller, kuangdd, wangzy23, hiyouga}@buaa.edu.cn, {niezj, zhangrc}@act.buaa.edu.cn  \n",
       "\n",
       "Abstract  \n",
       "This paper presents EasyRAG, a simple, lightweight, and efficient retrieval-augmented generation framework for automated network operations. Our framework has three advantages. The first is accurate question answering. We designed a straightforward RAG scheme based on (1) a specific data processing workflow (2) dual-route sparse retrieval for coarse ranking (3) LLM Reranker for reranking (4) LLM answer generation and optimization. This approach achieved first place in the GLM4 track in the preliminary round and second place in the GLM4 track in the semifinals. The second is simple deployment. Our method primarily consists of BM25 retrieval and BGE-reranker reranking, requiring no fine-tuning of any models, occupying minimal VRAM, easy to deploy, and highly scalable; we provide a flexible code library with various search and generation strategies, facilitating custom process implementation. The last one is efficient inference. We designed an efficient inference acceleration scheme for the entire coarse ranking, reranking, and generation process that significantly reduces the inference latency of RAG while maintaining a good level of accuracy; each acceleration scheme can be plug-and-play into any component of the RAG process, consistently enhancing the efficiency of the RAG system. Our code and data are released at https://github.com/BUAADreamer/EasyRAG.\n",
       "\n",
       "1 Introduction  \n",
       "Our solution can be summarized by Fig. 1, which includes a data processing workflow (Section 1.1) and the RAG process (Section 1.2).  \n",
       "*Corresponding author  \n",
       "'This work is a technical report of our solution at the 2024 (7th) CCF International AIOps Challenge. The official website of the Challenge is https://competition.aiops-challenge.com\n",
       "\n",
       "1.1 Ingestion  \n",
       "1.1.1 zedx file processing  \n",
       "Due to the discovery that the original processing script missed some files, we have reprocessed the zedx files using the following steps:  \n",
       "1. zedx Decompression: Decompress the official source data from four .zedx files, obtaining four packages of HTML documents.  \n",
       "2. Path Parsing: Read the knowledge path and the actual file path from the nodetree.xml in each document package.  \n",
       "3. Document Extraction: Extract the text, image titles, and image paths from each HTML document using BeautifulSoup.  \n",
       "4. Saving: Save the document text in txt format, maintaining the relative location consistent with the HTML document. Also, save the knowledge path, file path, and image path information.  \n",
       "\n",
       "1.1.2 Text Segmentation  \n",
       "Segmentation Settings: We used SentenceSplitter for document segmentation, initially splitting into sentences using Chinese punctuation, then merging according to the set text block size. The used block size (chunk-size) is 1024, and the block overlap size (chunk-overlap) is 200.  \n",
       "\n",
       "Eliminating Path Influence in Segmentation: In practice, we found that the original implementation of llama-index used a simple but unstable method of handling path information, subtracting the file path length from the text length to determine the actual text length used. This approach could cause different segmentation results with the same chunk-size and chunk-overlap, depending on the data path. During the preliminary competition, we observed that changing paths could lead to a fluctuation of..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. Ingestion\n",
       "   - Documents\n",
       "\n",
       "2. EasyRAG\n",
       "   - question\n",
       "   - bm25\n",
       "   - query rewrite\n",
       "   - split\n",
       "   - chunks:\n",
       "     - chunk 1\n",
       "     - chunk 2\n",
       "   - chunk retrieval\n",
       "   - metadata:\n",
       "     - file path\n",
       "   - Rerank Doc Embed / Retrieval filter\n",
       "   - knowledge path --> Retrieval Doc Embed\n",
       "   - extract:\n",
       "     - LLM Context Embed\n",
       "     - image content\n",
       "     - generate describe\n",
       "     - paddleocr\n",
       "     - filtered images\n",
       "     - image path\n",
       "     - word rule\n",
       "     - sentence rule\n",
       "     - glm4v-96-chat\n",
       "     - filter\n",
       "   - chunk 1\n",
       "   - chunk n-1\n",
       "   - chunk k\n",
       "   - chunk n\n",
       "   - bge- -> reranker-v2-\n",
       "   - minicom\n",
       "   - rerank\n",
       "   - generate\n",
       "   - GLM-4\n",
       "   - Ans':\n",
       "   - path retrieval\n",
       "   - chunk ntm\n",
       "   - query <--\n",
       "   - refine\n",
       "     - GLM-4\n",
       "     - Ans\n",
       "\n",
       "Figure 1: EasyRAG Framework\n",
       "\n",
       "Up to 3 percentage points in the final evaluation results is unacceptable in practice. To address this issue, we implemented a custom segmentation class that eliminates the use of path length, ensuring stable reproducibility.\n",
       "\n",
       "1.1.3 Image Information Extraction\n",
       "- Image Content Extraction Using a Multimodal Large Model\n",
       "  - We extracted information from all images using GLM-4V-9B (GLM et al., 2024). The simple prompt that achieves good results is: \n",
       "    - Briefly describe the image.\n",
       "  \n",
       "- Image Filtering Based on Various Rules\n",
       "  - We found that a small number of images are beneficial for the final question answering, but not all images are useful. Therefore, we designed a flexible strategy to filter out useless images using the following steps:\n",
       "    1. Use the PP-OCRv4 model to extract text content from images and filter out images that do not contain Chinese.\n",
       "    2. Filter images whose titles contain specific keywords (e.g., network diagrams, architecture).\n",
       "    3. Filter images that are referenced in the text in a specific way (e.g., configuration as shown in Figure x, file as shown in Figure x).\n",
       "  \n",
       "With these filtering steps, we reduced the number of images from an original 6000 to fewer than 200. The filtering process is easily configurable, allowing for tuning to suit real-world scenarios.\n",
       "\n",
       "1.2 RAG Pipeline\n",
       "1.2.1 Query Rewriting\n",
       "- Given the queries were very brief, we identified issues with some queries being semantically awkward or having unclear keywords. For instance, \"What types of alarms are there in EMSPLUS?\" and \"What are the sources of faults?\". Before inputting these queries into the RAG Pipeline, we used a Large Language Model (LLM, GLM4) for query rewriting, which involved two methods: query expansion and Hypothetical Document Embedding (HyDE) (Gao et al., 2022).\n",
       "\n",
       "Query Expansion\n",
       "- During the preliminary round, we summarized the characteristics of queries in the current operational maintenance scenario:\n",
       "  - Technical keywords in queries are crucial.\n",
       "  - Queries are short and vary greatly in the amount of information provided.\n",
       "\n",
       "In this context, we attempted to summarize the key terms in the queries or other potentially relevant keywords using the LLM, i.e., using the LLM's knowledge for keyword association and summary in the fields of operation and communication. This is referred to as keyword expansion."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "After manually annotating several data points with keywords and potential associations, we utilized the LLM (GLM4) for few-shot keyword summarization and expansion. Following (Wang et al., 2023), we generated new queries by directly concatenating the expanded keywords with the original query and then re-summarizing them using a large language model.\n",
       "\n",
       "Let L represent the Large Language Model LLM, with q and p denoting the initial query and the prompt, respectively. Pexp represents the expanded query prompt, including manually annotated data points, and psum represents the prompt for summarizing and concatenating the sentence and expanded keywords using the large model.\n",
       "\n",
       "In situations where queries lack specificity or identifiable elements, making it difficult for both dense and sparse retrieval methods to locate the target document, we designed a set of hypothetical document embedding methods, inspired by (Gao et al., 2022).\n",
       "\n",
       "For the generation of fictional documents, we devised two approaches: \n",
       "1. Following the paper's methodology, we input the prompt phy and the original question q into the large language model L to produce the fictional document g0. \n",
       "2. During the semifinals, we discovered that such fictional documents contained a significant amount of irrelevant keywords and redundant information due to the large model's hallucinations, greatly affecting the effectiveness of the retrieval process. Therefore, we attempted to minimize the hallucinations and redundant information in the initial fictional document g0 by using the BM25 algorithm and dense retrieval (using GTE-QWEN encoding) to identify the most relevant top-1 document and use it for context prompting.\n",
       "\n",
       "For the generated fictional documents, we also adopted two application methods: \n",
       "1. Using the fictional document q' combined with the original document q for coarse ranking retrieval. \n",
       "2. Using only the fictional document q' combined with the original document q for re-ranking of retrieval results.\n",
       "\n",
       "### Dual-route Sparse Retrieval for Coarse Ranking\n",
       "\n",
       "In the sparse retrieval section, we utilized the BM25 algorithm to construct the retriever. The core idea of BM25 is based on term frequency (TF) and inverse document frequency (IDF), and it also incorporates document length information to calculate the relevance between the document and query q. Specifically, the BM25 retriever primarily consists of a Chinese tokenizer and a stopword list. We will introduce each component in detail.\n",
       "\n",
       "#### Chinese Tokenizer\n",
       "For the Chinese tokenizer, we used the widely known jieba Chinese tokenizer, which is lightweight and supports multi-threaded mode to accelerate tokenization and part-of-speech analysis. It allows for customization of word frequency or dictionaries to adjust tokenization preferences. We also attempted to customize the vocabulary; in the current 5G communication maintenance scenario, we chose a related IT field lexicon collected by Tsinghua University loaded into the tokenizer. However, the results in practice were mediocre, so we ultimately continued using the original jieba lexicon.\n",
       "\n",
       "#### Stopword List\n",
       "For the Chinese stopword list, we adopted the common Chinese stopword list collected by Harbin Institute of Technology as a reference for filtering out meaningless words during Chinese tokenization. By filtering out irrelevant words and special symbols, we improve the hit rate of valid keywords and increase the recall rate of correct documents.\n",
       "\n",
       "### Dual-route Retrieval\n",
       "The BM25 dual-route retrieval for coarse ranking consists of text block retrieval and path retrieval.\n",
       "1. **Text block retrieval:** Use BM25 to search the segmented text blocks, recalling the top 192 text blocks with a coarse ranking score greater than 0.\n",
       "2. **Path retrieval:** Considering that some questions are highly relevant to our extracted knowledge paths, such as the question \"How...\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "many types of VNF elasticity are there?\",\n",
       "where both VNF and elasticity can be directly found in related knowledge paths. Hence, we designed a path search using BM25 to search the knowledge paths, recalling the top 6 text blocks with a coarse ranking score greater than 0.\n",
       "\n",
       "Retrieval Process The BM25 retriever follows the document retrieval process below for a given query q:\n",
       "1. Document Expansion. For text block retrieval, we concatenate the file path and each text block together to serve as expanded documents for retrieval.\n",
       "2. Document Preprocessing. First, filter all documents (text blocks or paths) with stopwords, then use the Chinese tokenizer for tokenization, and pre-compute the IDF scores of the documents.\n",
       "3. Query Processing. Filter the query q with stopwords and perform Chinese tokenization.\n",
       "4. Similarity Recall. Count the keywords of query q and calculate the TF values of each document, compute the relevance scores based on TF and IDF values, and recall relevant documents based on scores.\n",
       "5. File Path Filtering. For text block retrieval, we use the file paths provided in the competition to compare metadata, filtering out text blocks from other sources.\n",
       "\n",
       "Dense Retrieval for Coarse Ranking\n",
       "In the dense retrieval section, we employed the gte-Qwen2-7B-instruct model developed by Alibaba (Li et al., 2023), which has achieved advanced results on the MTEB benchmark.\n",
       "\n",
       "Retrieval Process The dense retriever for a given query q follows the specific document retrieval process as outlined below:\n",
       "1. Document Expansion. We concatenate the file path with each text block to serve as expanded documents for retrieval.\n",
       "2. Document Encoding. All text blocks are input into the model to be encoded and the representations are stored in a Qdrant vector database.\n",
       "3. Query Encoding. Using a query prompt template, we transform q into an input suitable for the GTE model and encode it using the model.\n",
       "4. Similarity Recall. During retrieval, cosine similarity is used for matching, recalling the top 288 text blocks.\n",
       "5. File Path Filtering. Using the file paths provided in the competition, we employ a Qdrant filter to eliminate text blocks from other sources.\n",
       "\n",
       "LLM Reranker Re-ranking\n",
       "We utilized the bge-reranker-v2-minicpm-layerwise model (Chen et al., 2024), a LLM Reranker trained on a hybrid of multiple multilingual ranking datasets using MiniCPM-2B-dpo-bf16. This model exhibits advanced ranking performance in both Chinese and English and includes accompanying tool code, which can be conveniently fine-tuned for specific scenarios.\n",
       "\n",
       "Re-ranking Process The LLM-Reranker for a given query q and k' coarsely ranked text blocks follows the specific document ranking process as outlined below:\n",
       "1. Document Expansion. We concatenate the knowledge paths with each text block to serve as expanded documents for retrieval.\n",
       "2. Text Processing. Combine q with the k' text blocks to form k' query-document pairs, which are then input into the tokenizer to generate input data for the LLM.\n",
       "3. Similarity Ranking. The input data is fed into the LLM to obtain re-ranking scores for the query and each text block, and the blocks are sorted according to these scores. The highest ranked k (typically 6) text blocks are returned.\n",
       "\n",
       "Multi-route Ranking Fusion\n",
       "Fusion Algorithm Since we designed multiple routes for coarse retrieval, it is also necessary to design corresponding ranking fusion strategies. We primarily used two strategies: simple merging and Reciprocal Rank Fusion (RRF). The simple merging strategy directly de-duplicates and merges text blocks obtained from multiple routes. Reciprocal Rank Fusion sums the reciprocals of the ranks of the same document across multiple retrieval paths to compute the fusion score for re-ranking."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Coarse Ranking Fusion  \n",
       "The most straightforward use of ranking fusion is to merge the text blocks obtained from multi-route coarse retrieval into a single set of text blocks, which are then passed to the Reranker for re-ranking. In the semifinals, we used simple merging to combine results from two sparse retrieval routes.  \n",
       "\n",
       "Re-ranking Fusion  \n",
       "We can also perform fusion after coarse ranking and re-ranking for each route. In the preliminary rounds, we fused text blocks from sparse and dense retrieval routes. For these two routes, we designed three re-ranking fusion methods.  \n",
       "1. Use RRF to merge the results after coarse and fine ranking.  \n",
       "2. Input the text blocks from each route into the LLM to obtain respective answers, selecting the longer answer as the final one.  \n",
       "3. Input the text blocks from each route into the LLM to obtain respective answers and directly concatenate the answers from all routes.  \n",
       "\n",
       "1.2.6 LLM Answer Generation  \n",
       "In this section, we first concatenate the contents of the top 6 text blocks obtained from re-ranking using the following template to create a context string:  \n",
       "### Document 0: {chunk_i}  \n",
       "### Document 5: {chunk_i}  \n",
       "\n",
       "Note that the text blocks input into GLM4 here include concatenated image content, whereas the text blocks in the previous coarse and re-ranking processes did not include image content.  \n",
       "\n",
       "We then combine the context string and the question using the following question-and-answer template, and input it into GLM4 to obtain an answer:  \n",
       "The context information is as follows:  \n",
       "{context_str}  \n",
       "Please answer the following question based on the context information and not your own knowledge. Answers can be itemized. If the context does not contain relevant information, you may respond with \"uncertain\" and should not restate the context information:  \n",
       "{query_str}  \n",
       "Answer:  \n",
       "\n",
       "Additionally, we have designed other formats of question-and-answer templates. Inspired by Chain-of-Thought (Wei et al., 2022), we designed a Chain-of-Thought question-and-answer template (see Appendix A.2). Drawing from COSTAR (Teo, 2023), we designed a markdown format question-and-answer template (see Appendix A.1). To emphasize the importance of the top 1 document, we designed a focused question-and-answer template (see Appendix A.3). Related experimental results are discussed therein.  \n",
       "\n",
       "1.2.7 LLM Answer Optimization  \n",
       "Due to our observation that the LLM gives attention to each text block, which may result in the effective information from the top 1 text block not being fully utilized, we designed an answer integration prompt (see Appendix B). This prompt allows us to integrate and supplement the answers derived from the 6 text blocks using the top 1 text block, leading to the final answer.  \n",
       "\n",
       "2 Accuracy  \n",
       "2.1 Abbreviations Introduction  \n",
       "For ease of writing, we first introduce some important component identifiers.  \n",
       "Data ① represents the official processed txt data.  \n",
       "① represents our own processed version 0 txt data, which supplements some missing data compared to the official data.  \n",
       "② is similar to ① but each txt begins with a concatenated knowledge path.  \n",
       "③ represents our own processed version 1 txt data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| id | data | chunk       | coarse ranking | re-ranking | fusion | accuracy |\n",
       "|----|------|-------------|----------------|------------|--------|----------|\n",
       "| 0  | 0    | 1024,50    | 0,3            | -          | -      | 57.86    |\n",
       "| 1  | 0    | 1024,50    | 1,8            | -          | -      | 68.59    |\n",
       "| 2  | 0    | 1024,50    | 3,8            | -          | -      | 69.55    |\n",
       "| 3  | 0    | 1024,50    | 1,192          | 0,8        | -      | 73.73    |\n",
       "| 4  | 0    | 1024,50    | 1,256          | 0,8        | -      | 70.68    |\n",
       "| 5  | 0    | 1024,50    | 2,192          | 0,8        | -      | 69.25    |\n",
       "| 6  | 0    | 1024,50    | 1,288          | 2,8        | -      | 77.07    |\n",
       "| 7  | 1    | 1024,50    | 1,288          | 2,8        | -      | 77.51    |\n",
       "| 8  | 2    | 1024,50    | 1,288          | 2,8        | -      | 77.92    |\n",
       "| 9  | 2    | 1024,50    | 1,256          | 2,8        | -      | 78.49    |\n",
       "| 10 | 5    | 1024,50    | 3,192          | 2,6        | -      | 80.90    |\n",
       "| 11 | 2    | 1024,50    | 3,192          | 2,6        | -      | 81.38    |\n",
       "| 12 | 2    | 1024,100   | 3,192          | 3,6        | -      | 81.77    |\n",
       "| 13 | 2    | 1024,100   | 3,192          | 3,6        | -      | 81.88    |\n",
       "| 14 | 2    | 1024,200   | 3,192          | 3,6        | -      | 82.87    |\n",
       "| 15 | 2    | 1024,200   | 3,192          | 3,6        | -      | 82.97    |\n",
       "| 16 | 2    | 1024,200   | 4,288          | 3,6        | -      | 83.02    |\n",
       "| 17 | 2    | 1024,200   | 4,288          | 3,192      | (3),6  | 81.80    |\n",
       "| 18 | 2    | 1024,200   | 4,288          | 3,192      | -      | 82.50    |\n",
       "| 19 | 2    | 1024,200   | 4,288          | 3,192      | 3).6   | 83.45    |\n",
       "| 20 | 2    | 1024,200   | 4,288          | 3,192      | -      | 83.70    |\n",
       "| 21 | 2    | 1024,200   | 4,288          | 3,192      | -      | 84.38    |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=7\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "id | data      | chunk      | coarse ranking | re-ranking | fusion | image | answer merge | accuracy\n",
       "---|-----------|------------|----------------|------------|--------|-------|--------------|---------\n",
       "0  | 960,200   | 3,192      | (3).6          | -          | -      | -     | -            | 91.53\n",
       "1  | 960,200   | 4,288      | (3).6          | -          | -      | -     | -            | 88.40\n",
       "2  | 960,200   | 4,288, 3,192 | (3).6        | 2          | -      | -     | -            | 90.00\n",
       "3  | 1024,200  | 3,192      | (3).6          | -          | -      | -     | -            | 90.26\n",
       "4  | 1024,200  | 3,192      | (3).6          | -          | -      | -     | -            | 91.38\n",
       "5  | 1024,200  | 3,192      | (3).6          | -          | -      | -     | -            | 92.70\n",
       "6  | 1024,200  | 3,192.1    | (3).6          | -          | -      | -     | -            | 89.30\n",
       "7  | 1024,200  | 3,192.2    | (3).6          | -          | -      | -     | -            | 87.12\n",
       "8  | 1024,200  | 3,192.1    | (3).6          | -          | -      | -     | -            | 92.43\n",
       "9  | 1024,200  | 3,192.2    | (3).6          | -          | -      | -     | -            | 93.11\n",
       "10 | 1024,200  | 3,192.2    | (3).6          | -          | -      | -     | -            | 90.17\n",
       "11 | 1024,200  | 3,192.2    | (3).6          | -          | OCR Filter | -  | -            | 92.5\n",
       "12 | 1024,200  | 3,192.2    | (3).6          | -          | Rule Filter | -| -            | 94.24\n",
       "13 | 1024,200  | 3,192.2    | (5).6          | -          | Rule Filter | -| -            | 94.49\n",
       "14 | 1024,200  | 3,192.5    | (3).6          | -          | Rule Filter | -| document concat | 96.65\n",
       "15 | 1024,200  | 3,192.6    | (3).6          | -          | Rule Filter | -| prompt merge   | 95.72"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=8\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Method | Preliminary Accuracy\n",
       "--- | ---\n",
       "Original | 82.0\n",
       "Concat | 78.2\n",
       "Summary | 79.4\n",
       "\n",
       "Method | Semi-final Accuracy\n",
       "--- | ---\n",
       "Original | 92.7\n",
       "Retrieval+HyDE | 89.2\n",
       "rerank+HyDE | 88.2\n",
       "\n",
       "Prompt Type | Semi-final Accuracy\n",
       "--- | ---\n",
       "Normal QA Template | 94.49\n",
       "COT QA Template | 89.75\n",
       "Markdown Format QA Template | 92.27\n",
       "Focused QA Template | 93.51"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=9\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 Inference Latency  \n",
       "5.1 Standard Scheme  \n",
       "**Standard Time Delay**  \n",
       "In the semi-final's standard scheme, we set the batch size for re-ranking to 32, with the inference latency for a question being 26 seconds, of which document sorting takes 6 seconds, and calling GLM4 twice takes 20 seconds.  \n",
       "\n",
       "**Removing Answer Integration**  \n",
       "By eliminating the answer integration step and directly returning the top 6 generated answers, only one call to GLM4 is needed, reducing the inference latency to 16 seconds.  \n",
       "\n",
       "**Increasing Re-ranking Batch Size**  \n",
       "Increasing the batch size to 256 increases the GPU memory usage but can reduce the inference latency to 24 seconds.  \n",
       "\n",
       "**Full Process Acceleration Scheme**  \n",
       "Beyond simple optimization strategies, we have also designed a full process acceleration scheme, which will be introduced in the following three subsections. This scheme aims to reduce time costs at each step. Due to the instability of GLM4 outputs, all experiments in this section terminate after the first generation of answers, without the final answer integration step, allowing for a more rigorous comparison of the impact of various acceleration methods on performance.  \n",
       "\n",
       "5.2 BM25 Acceleration  \n",
       "Since our retrieval stage relies heavily on BM25 for keyword matching, we introduced the bm25s (Lù, 2024) library to optimize the speed of BM25 retrieval.  \n",
       "\n",
       "| Implementation  | Time (s) | Accuracy  |\n",
       "|-----------------|----------|-----------|\n",
       "| BM25Okapi       | 17       | 94.49     |\n",
       "| BM25s           | 0.05     | 94.24     |\n",
       "\n",
       "**Table 6:** Effects of BM25 acceleration on the test set. Time represents the total search time for 103 questions related to BM25, and accuracy represents the evaluation score of the final generated answers.  \n",
       "\n",
       "5.3 Reranker Acceleration  \n",
       "We used the bge-reranker-v2-minicpm-layerwise model developed by the Zhejiang University's Institute for AI (Chen et al., 2024) as the LLM Reranker. This model supports customization of the number of inference layers, allowing selection from 8-40 layers based on one's needs and resource constraints, thus reducing GPU memory overhead. In our preliminary experiments, we found that 28 layers performed slightly better than 40 layers, with a difference of about 0.2 points, consistent with the empirical research conclusions given in the original repository. Therefore, both the preliminary and semi-final accuracy experiments utilized 28 layers.  \n",
       "\n",
       "However, since the Reranker is time-consuming in practical inference, we considered whether fewer layers could be used to speed up the process. Classic early-exit techniques in BERT, such as FastBERT (Liu et al., 2020) and DeeBERT (Xin et al., 2020), use information entropy exceeding a threshold as the condition for early exit, which is computationally intensive and results in unstable effects. Therefore, we designed a model early-exit algorithm based on maximum similarity selection; for each query, we check if the softmax similarity output at the 12th layer in the first batch contains any values exceeding a certain threshold; if so, this query is inferred using just 12 layers, otherwise 28 layers are used.  \n",
       "\n",
       "We conducted an experiment using an A100 40G GPU to explore inference time, GPU memory usage, and accuracy at a batch size of 32, comparing different layers and early-exit methods. We randomly selected 10 queries and chose 192 text blocks for each, including 6 ground truth text blocks sorted using 28 layers in the complete RAG and 186 other random blocks.  \n",
       "\n",
       "We predicted the sum of softmax scores of ground truth blocks relative to all blocks using various methods. Then, we assessed the similarity accuracy by dividing the predicted proportion by the proportion obtained with 28 layers and compared the ranking accuracy of predicted ground truth with the 28-layer results.  \n",
       "\n",
       "It can be seen that our proposed model early-exit method, while reducing inference time by 33%, is able to maintain ranking results consistent with those obtained using 28 layers directly, surpassing the entropy selection methods.  \n",
       "\n",
       "5.4 Context Compression  \n",
       "We designed a context compression method based on BM25 semantic similarity, which we call BM25-Extract. For each chunk, we first split it into sentences, then use BM25 to calculate the similarity between the query and each chunk, and finally add sentences to the list in order of decreasing similarity until a set compression rate is reached. The sentences are then concatenated in their original order."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=10\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Method | Time(s) | Similarity (%) | Rank\n",
       "--- | --- | --- | ---\n",
       "8-layer | 1.67 | 73 | 2.5\n",
       "12-layer | 2.20 | 88 | 3.2\n",
       "20-layer | 3.58 | 86 | 4.0\n",
       "28-layer | 5.25 | 100 | 6.0\n",
       "40-layer | 7.71 | 100 | 5.4\n",
       "Maximum (0.1) | 2.59 | 90 | 3.7\n",
       "Maximum (0.2) | 3.55 | 96 | 4.5\n",
       "Maximum (0.4) | 4.57 | 97 | 5.4\n",
       "Entropy (0.2) | 2.74 | 89 | 3.4\n",
       "Entropy (0.4) | 3.37 | 91 | 3.6\n",
       "Entropy (0.6) | 4.01 | 91 | 4.0\n",
       "\n",
       "Compression Algorithm | Compression Rate (%) | Tokens Saved | Accuracy | Time (s)\n",
       "--- | --- | --- | --- | ---\n",
       "Original Content | 100 | 0 | 94.49 | 9.30\n",
       "LLMLingua(0.5) | 62.80 | 143k | 83.44 | 10.47\n",
       "LongLLMLingua(0.5) | 62.80 | 143k | 80.86 | 10.52\n",
       "BM25-Extract(0.5) | 65.92 | 160k | 86.48 | 8.16\n",
       "BM25-Extract(0.8) | 83.84 | 59k | 89.00 | 7.70"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=11\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. 2020. FastBERT: a self-distilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6035-6044, Online. Association for Computational Linguistics.  \n",
       "Xing Han Lù. 2024. Bm25s: Orders of magnitude faster lexical search via eager sparse scoring. Preprint, arXiv:2407.03618.  \n",
       "Sheila Teo. 2023. How I won Singapore's GPT-4 prompt engineering competition.  \n",
       "Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with large language models. arXiv preprint arXiv:2303.07678.  \n",
       "\n",
       "## A Question-and-Answer Prompt Templates  \n",
       "\n",
       "### A.1 Markdown Format Question-and-Answer Template  \n",
       "## Objective  \n",
       "Please, based on the information from k private domain documents about 5G operational maintenance, answer the given question.  \n",
       "## Requirements  \n",
       "1. You may itemize your answer; be as detailed and specific as possible.  \n",
       "2. Do not merely repeat information from the context.  \n",
       "3. Do not use your own knowledge; rely solely on the content from the context documents.  \n",
       "## Context  \n",
       "{context_str}  \n",
       "## Question  \n",
       "{query_str}  \n",
       "## Answer  \n",
       "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.  \n",
       "\n",
       "### A.2 Chain of Thought Question-and-Answer Template  \n",
       "Context information as follows:  \n",
       "{context_str}  \n",
       "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. DeeBERT: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2246-2251, Online. Association for Computational Linguistics.  \n",
       "Please answer the following question based on the context information rather than your own knowledge. Think step by step, first provide an analysis process, then generate an answer:  \n",
       "{query_str}  \n",
       "Answer:  \n",
       "11"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page=12\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A.3 Focused Question-and-Answer Template  \n",
       "Context information as follows:  \n",
       "{context_str}  \n",
       "Please answer the following question based on the context information rather than your own knowledge. You may itemize your answer. Document 0's content is particularly important, consider it carefully. If the context does not contain relevant knowledge, you may respond with 'uncertain'. Do not simply restate the context information:  \n",
       "{query_str}  \n",
       "Answer:  \n",
       "\n",
       "B Answer Integration Template  \n",
       "Context:  \n",
       "{top1_content_str}  \n",
       "You will see a question and a corresponding reference answer. Please, based on the context knowledge and not your own knowledge, supplement the reference answer to make it more complete in addressing the question. Please note, strictly retain every character of the reference answer and reasonably integrate your supplement with the reference answer to produce a longer, more complete answer containing more terms and itemization.  \n",
       "Question:  \n",
       "{query_str}  \n",
       "Reference answer:  \n",
       "{answer_str}  \n",
       "New answer:  \n",
       "12"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gcp_config = GCPConfig(\n",
    "    GCP_PROJECT_ID=GCP_PROJECT_ID,\n",
    "    GCP_LOCATION=GCP_LOCATION,\n",
    "    GCP_PROCESSOR_ID=GCP_PROCESSOR_ID,\n",
    "    GCP_CREDENTIAL_PATH=GCP_CREDENTIAL_PATH\n",
    ")\n",
    "dai = DocumentAI(file_path, gcp_config)\n",
    "\n",
    "pages = len(PdfReader(file_path).pages)\n",
    "page_splits = [range(i, min(i+14, pages+1)) for i in range(1, pages+1, 15)]\n",
    "\n",
    "for page_split in page_splits:\n",
    "    result = dai.run_ocr(page_split)\n",
    "    document = result.document\n",
    "\n",
    "    dp = DocumentParser(document)\n",
    "    \n",
    "    for page in page_split:\n",
    "        content = dp.get_content_by_page(page)\n",
    "        image = dp.get_image_by_page(page)\n",
    "        image = transform_to_base64(image)\n",
    "\n",
    "        print(f\"{page=}\")\n",
    "        display(Markdown(refine(content, image)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
